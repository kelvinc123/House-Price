{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from importlib import reload\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the preprocessing step are done \n",
    "from _lib.preprocess import preprocess_missing as prep\n",
    "from _lib.preprocess import get_instruction as info\n",
    "from _lib.preprocess_test import preprocess_missing as prep_test\n",
    "from _lib.create_output import output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"_database/Input/train.csv\", index_col = 0)\n",
    "df = prep(df)\n",
    "df = df.drop([1299, 935, 186, 347, 1231, 1183, 692, 955])\n",
    "X_data = df.drop(\"SalePrice\", axis = 1)\n",
    "y = df[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate polynomial features of numerical variables\n",
    "def gen_poly(data, degree):\n",
    "    '''\n",
    "    Input : Vector or matrix \n",
    "    \n",
    "    Return matrix of polynomial for each polynomial degree from 1 to degree calculated on each column\n",
    "    '''\n",
    "    \n",
    "    result = np.concatenate([np.power(data, d) for d in np.arange(1, degree + 1)], axis = 1)\n",
    "    return result\n",
    "\n",
    "df_type = pd.DataFrame({\"Columns\" : X_data.columns, \"Type\" : [str(X_data[col].dtype) for col in X_data.columns]})\n",
    "num_columns = df_type.loc[(df_type[\"Type\"] == \"int64\") | (df_type[\"Type\"] == \"float64\")][\"Columns\"]\n",
    "cat_columns = df_type.loc[(df_type[\"Type\"] == \"category\")][\"Columns\"]\n",
    "bool_columns = df_type.loc[df_type[\"Type\"] == \"bool\"][\"Columns\"]\n",
    "\n",
    "# Categories in categorical features\n",
    "list_categories = [np.array(info(col)) for col in cat_columns.values]\n",
    "list_categories[14] = np.arange(1, 11)\n",
    "list_categories[15] = np.arange(1, 11)\n",
    "\n",
    "\n",
    "# Polynomial degree\n",
    "poly_degree = 1\n",
    "\n",
    "get_numerical = FunctionTransformer(lambda x : x[num_columns.values].values, validate = False)\n",
    "\n",
    "get_category = FunctionTransformer(lambda x : x[cat_columns.values], validate = False)\n",
    "\n",
    "get_bool = FunctionTransformer(lambda x : x[bool_columns.values].values, validate = False)\n",
    "\n",
    "generate_poly = FunctionTransformer(lambda x : gen_poly(x, poly_degree), validate = False)\n",
    "\n",
    "pipeline_num_prep = Pipeline([('selector', get_numerical),\n",
    "                              ('poly', generate_poly)])\n",
    "\n",
    "pipeline_cat_prep = Pipeline([('selector', get_category),\n",
    "                              ('Dummy', OneHotEncoder(drop = 'first', sparse = False,\n",
    "                                                     categories = list_categories))])\n",
    "\n",
    "transformers = [ ('Numerical', pipeline_num_prep), ('Categorical', pipeline_cat_prep), ('Bool', get_bool) ]\n",
    "\n",
    "preprocess_union = FeatureUnion(transformer_list = transformers)\n",
    "\n",
    "pl = Pipeline([\n",
    "    ('union', preprocess_union)\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pl.fit_transform(X_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're gonna transform $y$ to $log(y)$ so that $y$ is normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we split the data into training set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, random_state = 45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first quick ML, we use Linear Regression. We can use this model as our baseline model to compare with another ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ml.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mse(estimator, mse):\n",
    "    print(\"The MSE for {} : {}\".format(estimator, mse))\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE for Linear Regression : 0.02068383498033706\n"
     ]
    }
   ],
   "source": [
    "print_mse(\"Linear Regression\", mean_squared_error(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\text{MSE}$ formula can be given by : \n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n} \\sum_{i = 1}^{n} (y_{val}^{(i)} - \\hat{y}_{val}^{(i)})^{2} $$\n",
    "\n",
    "with $n = $ length of $y_{val}$\n",
    "\n",
    "Keep in mind that we've transformed $y \\rightarrow log(y)$, so the $\\text{MSE}$ can be that small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using poly degree > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_reg_pipeline(poly_degree):\n",
    "\n",
    "    get_numerical = FunctionTransformer(lambda x : x[num_columns.values].values, validate = False)\n",
    "\n",
    "    get_category = FunctionTransformer(lambda x : x[cat_columns.values], validate = False)\n",
    "\n",
    "    get_bool = FunctionTransformer(lambda x : x[bool_columns.values].values, validate = False)\n",
    "\n",
    "    generate_poly = FunctionTransformer(lambda x : gen_poly(x, poly_degree), validate = False)\n",
    "\n",
    "    pipeline_num_prep = Pipeline([('selector', get_numerical),\n",
    "                                  ('poly', generate_poly)])\n",
    "\n",
    "    pipeline_cat_prep = Pipeline([('selector', get_category),\n",
    "                                  ('Dummy', OneHotEncoder(drop = 'first', sparse = False,\n",
    "                                                         categories = list_categories))])\n",
    "\n",
    "    transformers = [ ('Numerical', pipeline_num_prep), ('Categorical', pipeline_cat_prep), ('Bool', get_bool) ]\n",
    "\n",
    "    preprocess_union = FeatureUnion(transformer_list = transformers)\n",
    "\n",
    "    pl = Pipeline([\n",
    "        ('union', preprocess_union)\n",
    "\n",
    "    ])\n",
    "    \n",
    "    return pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to try fitting the linear regression model with polynomial degree. We use for loop to find the best polynomial degree by looking at the lowest $\\text{MSE}_{val}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE for poly degree 1 is : 0.02068383498033706\n",
      "The MSE for poly degree 2 is : 0.021234706759052556\n",
      "The MSE for poly degree 3 is : 0.022998001827133612\n",
      "The MSE for poly degree 4 is : 0.04879593474584603\n",
      "The MSE for poly degree 5 is : 0.09241880319826536\n",
      "The MSE for poly degree 6 is : 0.04608764699637089\n",
      "The MSE for poly degree 7 is : 0.07714470254798017\n",
      "The MSE for poly degree 8 is : 0.11031185681483113\n",
      "The MSE for poly degree 9 is : 0.15554147233420523\n",
      "The MSE for poly degree 10 is : 0.15575008071655275\n",
      "The MSE for poly degree 11 is : 0.1557658666230213\n",
      "The MSE for poly degree 12 is : 0.15572648795675428\n",
      "The MSE for poly degree 13 is : 0.155689114397073\n",
      "The MSE for poly degree 14 is : 0.15566300298434696\n",
      "The MSE for poly degree 15 is : 0.15572687908578128\n",
      "The MSE for poly degree 16 is : 0.1557016433600588\n",
      "The MSE for poly degree 17 is : 0.1557279510591014\n",
      "The MSE for poly degree 18 is : 0.15572900301843212\n",
      "The MSE for poly degree 19 is : 0.15577839201416663\n"
     ]
    }
   ],
   "source": [
    "n = 20\n",
    "seed = 45\n",
    "msle_list = []\n",
    "for i in range(1, n):\n",
    "    pl = poly_reg_pipeline(i)\n",
    "    X = pl.fit_transform(X_data)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, random_state = seed)\n",
    "    ml = LinearRegression()\n",
    "    \n",
    "    ml.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = ml.predict(X_val)\n",
    "    msle = mean_squared_error(y_val, y_pred)\n",
    "    msle_list.append(msle)\n",
    "    \n",
    "    print(\"The MSE for poly degree {} is : {}\".format(i, msle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result shows us that the best polynomial degree is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression parameters $\\theta$ can be computed by minimizing cost function $J(\\theta)$ with formula :\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2n} \\sum_{i = 1}^{n} (\\hat{y}^{(i)} - y^{(i)})^{2}$$\n",
    "\n",
    "Ridge regression calculates $\\theta$ by minimizing : \n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2n} \\sum_{i = 1}^{n} (\\hat{y}^{(i)} - y^{(i)})^{2} + \\frac{\\lambda}{2n}\\sum_{j = 1}^{k} \\theta_{k}^{2}$$\n",
    "\n",
    "By adding regularization term , we can reduce the value of $\\theta$. This will result in a **more bias** estimator but it has a **lower variance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = poly_reg_pipeline(1)\n",
    "X = pl.fit_transform(X_data)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, random_state = 45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Ridge and Lasso regression in scikit-learn, there is a regularization parameter $C$. We want to find the best $C$ for our model by comparing several values of $C$ with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = np.logspace(-3, 1, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = np.zeros(len(alpha_list))\n",
    "for i in np.arange(len(alpha_list)):\n",
    "    ml = Ridge(alpha = alpha_list[i])\n",
    "    ml.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = ml.predict(X_val)\n",
    "    \n",
    "    mse[i] = mean_squared_error(y_val, y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Plot of C vs. MSE')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(alpha_list, mse)\n",
    "ax.set_ylabel(\"MSE\")\n",
    "ax.set_xlabel(\"C\")\n",
    "ax.set_title(\"Plot of C vs. MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = (np.arange(1, len(mse) + 1).reshape(1, len(mse))@((mse == np.min(mse)).reshape(len(mse), 1) * 1)).reshape(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.04811313])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_list[best_idx - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the best $C$ on the bigger range of values, we can use the function GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list1 = np.logspace(-3, 2, 50)\n",
    "alpha_list2 = np.array([0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000])\n",
    "alpha_list = np.concatenate((alpha_list1, alpha_list2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'alpha' : alpha_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = GridSearchCV(ml, param_grid = params, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score=nan,\n",
       "             estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True,\n",
       "                             max_iter=None, normalize=False, random_state=None,\n",
       "                             solver='auto', tol=0.001),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'alpha': array([1.00000000e-03, 1.26485522e-03, 1.59985872e-03, 2.02358965e-03,\n",
       "       2.55954792e-03, 3.23745754e-03, 4.09491506e-03, 5.17947468e-03,\n",
       "       6.5512855...\n",
       "       3.08884360e+01, 3.90693994e+01, 4.94171336e+01, 6.25055193e+01,\n",
       "       7.90604321e+01, 1.00000000e+02, 0.00000000e+00, 1.00000000e-03,\n",
       "       3.00000000e-03, 1.00000000e-02, 3.00000000e-02, 1.00000000e-01,\n",
       "       3.00000000e-01, 1.00000000e+00, 3.00000000e+00, 1.00000000e+01,\n",
       "       3.00000000e+01, 1.00000000e+02, 3.00000000e+02, 1.00000000e+03])},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 9.540954763499943}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ml.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE for Ridge Regression : 0.01838296139171516\n"
     ]
    }
   ],
   "source": [
    "print_mse(\"Ridge Regression\", mean_squared_error(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are better than the linear regression, one of the reason is we have a huge number of features. Even though we didn't use polynomial, we still suffer from high variance which can be fixed by using ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso regression find the parameter $\\theta$ by minimizing :\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2n} \\sum_{i = 1}^{n} (\\hat{y}^{(i)} - y^{(i)})^{2} + \\frac{\\lambda}{2n}\\sum_{j = 1}^{k} |\\theta_{k}|$$\n",
    "\n",
    "The difference between lasso and ridge is lasso allows the parameter to be 0, so it can also perform feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = Lasso()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = GridSearchCV(ml, param_grid = params, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score=nan,\n",
       "             estimator=Lasso(alpha=1.0, copy_X=True, fit_intercept=True,\n",
       "                             max_iter=1000, normalize=False, positive=False,\n",
       "                             precompute=False, random_state=None,\n",
       "                             selection='cyclic', tol=0.0001, warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'alpha': array([1.00000000e-03, 1.26485522e-03, 1.59985872e-03, 2.02358965e-03,\n",
       "       2.55954792e-03...\n",
       "       3.08884360e+01, 3.90693994e+01, 4.94171336e+01, 6.25055193e+01,\n",
       "       7.90604321e+01, 1.00000000e+02, 0.00000000e+00, 1.00000000e-03,\n",
       "       3.00000000e-03, 1.00000000e-02, 3.00000000e-02, 1.00000000e-01,\n",
       "       3.00000000e-01, 1.00000000e+00, 3.00000000e+00, 1.00000000e+01,\n",
       "       3.00000000e+01, 1.00000000e+02, 3.00000000e+02, 1.00000000e+03])},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_param = cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ml.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE for Lasso Regression : 0.01826856054435967\n"
     ]
    }
   ],
   "source": [
    "print_mse(\"Lasso Regression\", mean_squared_error(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is slightly better than ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=None,\n",
       "                      max_features=None, max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                      random_state=None, splitter='best')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
       "                      random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'min_samples_leaf' : np.linspace(0.1, 1, 50),\n",
    "         'max_depth' : np.linspace(1, 300, 100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = GridSearchCV(ml, param_grid = params, cv = 10, n_jobs=-1, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 5000 candidates, totalling 50000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 2380 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done 6380 tasks      | elapsed:   31.1s\n",
      "[Parallel(n_jobs=-1)]: Done 11980 tasks      | elapsed:   57.8s\n",
      "[Parallel(n_jobs=-1)]: Done 19180 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 25636 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 30382 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 35332 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 45756 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 50000 out of 50000 | elapsed:  5.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score=nan,\n",
       "             estimator=DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse',\n",
       "                                             max_depth=None, max_features=None,\n",
       "                                             max_leaf_nodes=None,\n",
       "                                             min_impurity_decrease=0.0,\n",
       "                                             min_impurity_split=None,\n",
       "                                             min_samples_leaf=1,\n",
       "                                             min_samples_split=2,\n",
       "                                             min_weight_fraction_leaf=0.0,\n",
       "                                             presort='deprecated',\n",
       "                                             random_state=None,\n",
       "                                             splitter='best'),\n",
       "             iid='deprecated', n_jobs=-1...\n",
       "       0.55918367, 0.57755102, 0.59591837, 0.61428571, 0.63265306,\n",
       "       0.65102041, 0.66938776, 0.6877551 , 0.70612245, 0.7244898 ,\n",
       "       0.74285714, 0.76122449, 0.77959184, 0.79795918, 0.81632653,\n",
       "       0.83469388, 0.85306122, 0.87142857, 0.88979592, 0.90816327,\n",
       "       0.92653061, 0.94489796, 0.96326531, 0.98163265, 1.        ])},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 179.1919191919192, 'min_samples_leaf': 0.1}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ml.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE for Decision Tree : 0.05903539787727484\n"
     ]
    }
   ],
   "source": [
    "print_mse(\"Decision Tree\", mean_squared_error(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameter for Decision tree has higher $\\text{MSE}$ than lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'min_samples_leaf' : np.linspace(0.0001, 0.001, 10),\n",
    "         'max_depth' : np.linspace(130, 140, 5),\n",
    "         'n_estimators' : np.arange(33, 39)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = GridSearchCV(ml, param_grid = params, n_jobs=-1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   13.4s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   45.8s\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1450 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed:  7.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=RandomForestRegressor(bootstrap=True, ccp_alpha=0.0,\n",
       "                                             criterion='mse', max_depth=None,\n",
       "                                             max_features='auto',\n",
       "                                             max_leaf_nodes=None,\n",
       "                                             max_samples=None,\n",
       "                                             min_impurity_decrease=0.0,\n",
       "                                             min_impurity_split=None,\n",
       "                                             min_samples_leaf=1,\n",
       "                                             min_samples_split=2,\n",
       "                                             min_weight_fraction_leaf=0.0,\n",
       "                                             n_estimators=100, n_jobs=None,\n",
       "                                             oob_score=False, r..._state=None,\n",
       "                                             verbose=0, warm_start=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'max_depth': array([130. , 132.5, 135. , 137.5, 140. ]),\n",
       "                         'min_samples_leaf': array([0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008,\n",
       "       0.0009, 0.001 ]),\n",
       "                         'n_estimators': array([33, 34, 35, 36, 37, 38])},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=2)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 135.0, 'min_samples_leaf': 0.0007, 'n_estimators': 37}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ml.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE for Random Forest : 0.022077014052012452\n"
     ]
    }
   ],
   "source": [
    "print_mse(\"Random Forest\", mean_squared_error(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the $\\text{MSE}$ of our models, we choose Lasso regression as our machine learning model to predict log house price sales since it has the lowest validation $\\text{MSE}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"_database/Input/test.csv\", index_col = 0)\n",
    "test_index = test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = prep_test(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pl.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = Lasso() # our choice of model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're using all of our training data to fit the model, so we need to retune the hyperparameter $C$ using all of our training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'alpha' : alpha_list} # using the alpha_list defined earlier\n",
    "cv = GridSearchCV(ml, param_grid = params, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score=nan,\n",
       "             estimator=Lasso(alpha=1.0, copy_X=True, fit_intercept=True,\n",
       "                             max_iter=1000, normalize=False, positive=False,\n",
       "                             precompute=False, random_state=None,\n",
       "                             selection='cyclic', tol=0.0001, warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'alpha': array([1.00000000e-03, 1.26485522e-03, 1.59985872e-03, 2.02358965e-03,\n",
       "       2.55954792e-03...\n",
       "       3.08884360e+01, 3.90693994e+01, 4.94171336e+01, 6.25055193e+01,\n",
       "       7.90604321e+01, 1.00000000e+02, 0.00000000e+00, 1.00000000e-03,\n",
       "       3.00000000e-03, 1.00000000e-02, 3.00000000e-02, 1.00000000e-01,\n",
       "       3.00000000e-01, 1.00000000e+00, 3.00000000e+00, 1.00000000e+01,\n",
       "       3.00000000e+01, 1.00000000e+02, 3.00000000e+02, 1.00000000e+03])},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit(X, y) # using full training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we first use log to transform y, we need to restore it using exp\n",
    "y_pred = np.exp(ml.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our module to create output, we made csv files in the output folder contains our prediction of house prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a filename : lasso_prediction\n",
      "lasso_prediction.csv was generated\n"
     ]
    }
   ],
   "source": [
    "output(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
